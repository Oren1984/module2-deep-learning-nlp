{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Performance (Module 2 — Deep Learning & NLP)\n",
    "\n",
    "Goal: benchmark CPU vs accelerator (CUDA/MPS) for a few common ops used in DL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What this notebook checks\n",
    "- Python & OS info\n",
    "- PyTorch installation and version\n",
    "- GPU/accelerator availability (CUDA / MPS)\n",
    "- A tiny forward pass to verify basic functionality\n",
    "- Reproducibility seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.9\n",
      "Platform: Windows-11-10.0.26200-SP0\n",
      "PyTorch: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# If you see \"No module named torch\", install requirements first:\n",
    "# pip install -r requirements.txt\n",
    "import os, sys, platform, time\n",
    "import torch\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Platform:', platform.platform())\n",
    "print('PyTorch:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    # Apple Silicon\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print('Selected device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('CUDA device:', torch.cuda.get_device_name(0))\n",
    "    print('CUDA capability:', torch.cuda.get_device_capability(0))\n",
    "    print('CUDA version (runtime):', torch.version.cuda)\n",
    "    print('cuDNN enabled:', torch.backends.cudnn.enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set ✅\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "print('Seed set ✅')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark helpers\n",
    "\n",
    "We use `torch.cuda.synchronize()` when on CUDA to get accurate timings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready ✅\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from time import perf_counter\n",
    "\n",
    "def sync():\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def timeit(fn, warmup=10, iters=50):\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    sync()\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    sync()\n",
    "    t1 = perf_counter()\n",
    "    return (t1 - t0) / iters\n",
    "\n",
    "print('Ready ✅')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Matrix multiplication (GEMM)\n",
    "\n",
    "This is a core operation behind many layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU matmul avg: 13.42 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def bench_matmul(n=2048, dtype=torch.float32):\n",
    "    a_cpu = torch.randn(n, n, dtype=dtype)\n",
    "    b_cpu = torch.randn(n, n, dtype=dtype)\n",
    "\n",
    "    def run_cpu():\n",
    "        _ = a_cpu @ b_cpu\n",
    "\n",
    "    cpu_s = timeit(run_cpu, warmup=3, iters=10)\n",
    "\n",
    "    if device.type in ('cuda','mps'):\n",
    "        a = a_cpu.to(device)\n",
    "        b = b_cpu.to(device)\n",
    "        def run_dev():\n",
    "            _ = a @ b\n",
    "        dev_s = timeit(run_dev, warmup=10, iters=50)\n",
    "        return cpu_s, dev_s\n",
    "    return cpu_s, None\n",
    "\n",
    "cpu_s, dev_s = bench_matmul(n=1024)\n",
    "print(f'CPU matmul avg: {cpu_s*1000:.2f} ms')\n",
    "if dev_s is not None:\n",
    "    print(f'{device.type.upper()} matmul avg: {dev_s*1000:.2f} ms')\n",
    "    print(f'Speedup: {cpu_s/dev_s:.2f}x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Convolution (Conv2D)\n",
    "\n",
    "Common in CNNs and also used in some NLP/CV hybrids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU conv avg: 6.26 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def bench_conv(batch=32, channels=3, h=224, w=224):\n",
    "    conv_cpu = nn.Conv2d(channels, 32, kernel_size=3, padding=1)\n",
    "    x_cpu = torch.randn(batch, channels, h, w)\n",
    "\n",
    "    def run_cpu():\n",
    "        _ = conv_cpu(x_cpu)\n",
    "    cpu_s = timeit(run_cpu, warmup=3, iters=10)\n",
    "\n",
    "    if device.type in ('cuda','mps'):\n",
    "        conv = conv_cpu.to(device)\n",
    "        x = x_cpu.to(device)\n",
    "        def run_dev():\n",
    "            _ = conv(x)\n",
    "        dev_s = timeit(run_dev, warmup=10, iters=50)\n",
    "        return cpu_s, dev_s\n",
    "    return cpu_s, None\n",
    "\n",
    "cpu_s, dev_s = bench_conv(batch=8, h=128, w=128)\n",
    "print(f'CPU conv avg: {cpu_s*1000:.2f} ms')\n",
    "if dev_s is not None:\n",
    "    print(f'{device.type.upper()} conv avg: {dev_s*1000:.2f} ms')\n",
    "    print(f'Speedup: {cpu_s/dev_s:.2f}x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Small training step\n",
    "\n",
    "A minimal training step (forward + backward) to reflect real training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU train-step avg: 29.99 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_step(device_to_use):\n",
    "    model = nn.Sequential(nn.Linear(512, 1024), nn.ReLU(), nn.Linear(1024, 10)).to(device_to_use)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    x = torch.randn(256, 512, device=device_to_use)\n",
    "    y = torch.randint(0, 10, (256,), device=device_to_use)\n",
    "\n",
    "    def step():\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return step\n",
    "\n",
    "cpu_step = train_step(torch.device('cpu'))\n",
    "cpu_s = timeit(cpu_step, warmup=2, iters=5)\n",
    "print(f'CPU train-step avg: {cpu_s*1000:.2f} ms')\n",
    "\n",
    "if device.type in ('cuda','mps'):\n",
    "    dev_step = train_step(device)\n",
    "    dev_s = timeit(dev_step, warmup=5, iters=20)\n",
    "    print(f'{device.type.upper()} train-step avg: {dev_s*1000:.2f} ms')\n",
    "    print(f'Speedup: {cpu_s/dev_s:.2f}x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation tips\n",
    "- If speedup is small, that can be normal for tiny models/batches.\n",
    "- Bigger batches and heavier models usually benefit more from GPU.\n",
    "- For stable results, close other heavy apps and run the notebook twice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".module2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
